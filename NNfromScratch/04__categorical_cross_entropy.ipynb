{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CategoricalCrossEntropyLoss:\n",
    "    def calculate(self, y_pred, y_true):\n",
    "        # Clip predictions to prevent division by zero\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "        \n",
    "        # Handle both categorical labels and one-hot encoded labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(len(y_pred)), y_true]\n",
    "        else:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "        \n",
    "        # Calculate negative log likelihoods and mean loss\n",
    "        losses = -np.log(correct_confidences)\n",
    "        return np.mean(losses)\n",
    "\n",
    "# Dry Run Example 1: One-Hot Encoded Labels\n",
    "# Input data\n",
    "class_targets = np.array([\n",
    "    [1, 0, 0],  # Class 0\n",
    "    [0, 1, 0],  # Class 1\n",
    "    [0, 1, 0]   # Class 1\n",
    "])\n",
    "\n",
    "softmax_outputs = np.array([\n",
    "    [0.7, 0.1, 0.2],  # Predicted probabilities for sample 1\n",
    "    [0.1, 0.5, 0.4],  # Predicted probabilities for sample 2\n",
    "    [0.02, 0.9, 0.08] # Predicted probabilities for sample 3\n",
    "])\n",
    "\n",
    "# Step-by-step dry run for one-hot encoded labels\n",
    "# 1. Clip predictions to avoid division by zero\n",
    "# y_pred_clipped = np.clip(softmax_outputs, 1e-7, 1-1e-7)\n",
    "# y_pred_clipped ≈ [[0.7, 0.1, 0.2],\n",
    "#                   [0.1, 0.5, 0.4],\n",
    "#                   [0.02, 0.9, 0.08]] (no significant change since values are within bounds)\n",
    "\n",
    "# 2. Check y_true shape: class_targets.shape = (3, 3), so it's 2D (one-hot encoded)\n",
    "# Use: correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "# 3. Calculate correct confidences (element-wise multiplication and sum along axis 1)\n",
    "# Sample 1: [0.7, 0.1, 0.2] * [1, 0, 0] = [0.7, 0, 0] → sum = 0.7\n",
    "# Sample 2: [0.1, 0.5, 0.4] * [0, 1, 0] = [0, 0.5, 0] → sum = 0.5\n",
    "# Sample 3: [0.02, 0.9, 0.08] * [0, 1, 0] = [0, 0.9, 0] → sum = 0.9\n",
    "# correct_confidences = [0.7, 0.5, 0.9]\n",
    "\n",
    "# 4. Calculate negative log likelihoods\n",
    "# losses = -np.log([0.7, 0.5, 0.9])\n",
    "#        ≈ [-np.log(0.7), -np.log(0.5), -np.log(0.9)]\n",
    "#        ≈ [0.3567, 0.6931, 0.1054]\n",
    "\n",
    "# 5. Calculate mean loss\n",
    "# mean_loss = np.mean([0.3567, 0.6931, 0.1054]) ≈ 0.3851\n",
    "\n",
    "# Example 1 Output:\n",
    "loss_obj = CategoricalCrossEntropyLoss()\n",
    "loss = loss_obj.calculate(softmax_outputs, class_targets)\n",
    "# loss ≈ 0.3851\n",
    "\n",
    "# Dry Run Example 2: Categorical Labels\n",
    "# Input data (same softmax_outputs, but class_targets as categorical indices)\n",
    "class_targets_categorical = np.array([0, 1, 1])  # Class indices: 0, 1, 1\n",
    "\n",
    "# Step-by-step dry run for categorical labels\n",
    "# 1. Clip predictions (same as above)\n",
    "# y_pred_clipped ≈ [[0.7, 0.1, 0.2],\n",
    "#                   [0.1, 0.5, 0.4],\n",
    "#                   [0.02, 0.9, 0.08]]\n",
    "\n",
    "# 2. Check y_true shape: class_targets_categorical.shape = (3,), so it's 1D (categorical)\n",
    "# Use: correct_confidences = y_pred_clipped[range(len(y_pred)), y_true]\n",
    "\n",
    "# 3. Calculate correct confidences (select probabilities at true class indices)\n",
    "# range(len(y_pred)) = [0, 1, 2]\n",
    "# y_true = [0, 1, 1]\n",
    "# correct_confidences = [y_pred_clipped[0,0], y_pred_clipped[1,1], y_pred_clipped[2,1]]\n",
    "#                    = [0.7, 0.5, 0.9]\n",
    "\n",
    "# 4. Calculate negative log likelihoods (same as above)\n",
    "# losses = -np.log([0.7, 0.5, 0.9])\n",
    "#        ≈ [0.3567, 0.6931, 0.1054]\n",
    "\n",
    "# 5. Calculate mean loss\n",
    "# mean_loss = np.mean([0.3567, 0.6931, 0.1054]) ≈ 0.3851\n",
    "\n",
    "# Example 2 Output:\n",
    "loss = loss_obj.calculate(softmax_outputs, class_targets_categorical)\n",
    "# loss ≈ 0.3851"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
